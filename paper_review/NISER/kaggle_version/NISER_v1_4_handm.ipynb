{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UYWCQTb_4YE7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651760457861,"user_tz":-540,"elapsed":2868,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}},"outputId":"9ca5e482-29e4-431c-fece-6e7c77ada736"},"execution_count":177,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","import sys\n","import numpy as np\n","from scipy import sparse\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import pandas as pd\n","import tensorflow as tf\n","import bottleneck as bn\n","import keras\n","import math"],"metadata":{"id":"9FDOE0wheiKr","executionInfo":{"status":"ok","timestamp":1651760446969,"user_tz":-540,"elapsed":14,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import time\n","import csv\n","import pickle\n","import operator\n","import datetime\n","import os"],"metadata":{"id":"V9O9hLXf5831","executionInfo":{"status":"ok","timestamp":1651760446970,"user_tz":-540,"elapsed":14,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab Notebooks/recommendation_system/deep_learning/session_based_rec/test/datasets/HandM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_2NZdf15U1D","executionInfo":{"status":"ok","timestamp":1651760459213,"user_tz":-540,"elapsed":7,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}},"outputId":"6b3f161b-3ed5-45be-e3ad-cfb55a68ae5e"},"execution_count":178,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/recommendation_system/deep_learning/session_based_rec/test/datasets/HandM\n"]}]},{"cell_type":"code","source":["tra=pickle.load(open('HandM/0506tra.txt', 'rb'))\n","tes=pickle.load(open('HandM/0506tes.txt', 'rb'))"],"metadata":{"id":"iKZtUrBoLSdT","executionInfo":{"status":"ok","timestamp":1651760468853,"user_tz":-540,"elapsed":9644,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":179,"outputs":[]},{"cell_type":"code","source":["def max_len_check(all_usr_pois):\n","\n","    us_lens = [len(upois) for upois in all_usr_pois] # 이게 오래 걸리는듯.\n","    #print(\"us_lens in data_masks\")\n","    #print(us_lens,'\\n')\n","    \n","    len_max = max(us_lens)\n","    return  len_max\n","\n","def split_validation(train_set, valid_portion):\n","    train_set_x, train_set_y = train_set\n","    n_samples = len(train_set_x)\n","    sidx = np.arange(n_samples, dtype='int32')\n","    np.random.shuffle(sidx)\n","    n_train = int(np.round(n_samples * (1. - valid_portion)))\n","    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n","    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n","    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n","    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n","\n","    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n","\n","\n","class Data():\n","    def __init__(self, data, one_day_max=100,shuffle=False, graph=None):\n","        inputs = data[0]\n","        len_max = max_len_check(inputs)\n","        self.inputs = inputs\n","        \n","        self.len_max = len_max\n","        self.targets = np.asarray(data[1])\n","        self.length = len(inputs)\n","        self.shuffle = shuffle\n","        self.graph = graph\n","        self.one_day_max = one_day_max\n","\n","    def generate_batch(self, batch_size):\n","        if self.shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.inputs = self.inputs[shuffled_arg]\n","            #self.mask = self.mask[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n","        return slices\n","\n","    def get_slice(self, i):\n","        #inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n","        inputs = []\n","        for jk in i :\n","          inputs.append(self.inputs[jk])\n","\n","        targets = self.targets[i]\n","\n","        us_lens = [len(upois) for upois in inputs] # 이게 오래 걸리는듯.\n","        \n","        \n","        inputs = [[j + [0] * (self.one_day_max - len(j)) for j in one_seq] for one_seq in inputs]\n","        \n","        \n","        for idx,le in enumerate(us_lens):\n","          for jj in range(self.len_max - le):\n","            inputs[idx].append(self.one_day_max * [0])\n","        \n","\n","        \n","        \n","        items, n_node, A, alias_inputs = [], [], [], []\n","        for u_input in inputs:\n","            n_node.append(len(np.unique(u_input))) \n","        max_n_node = np.max(n_node) # 해당 slices 중 한 sequence 안에서 가질 수 있는 아이템의 종류의 최댓값\n","        # 0 패딩 후에 개수를 따지는 것이기 때문에 id 개수 +1 임.\n","        \n","        \n","        \n","        for u_input in inputs:\n","            node = np.unique(u_input)\n","            \n","           \n","            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n","            \n","            u_A = np.zeros((max_n_node, max_n_node)) # 균일한 크기로 생성.\n","            \n","            \n","        \n","\n","            for i in range(len(u_input)):\n","              # same time point loop\n","              length=len(u_input[i])\n","              for j in range(length):\n","                for k in range(length):\n","                  if u_input[i][k] == 0 or  u_input[i][j] == 0:\n","                    break\n","                  u = np.where(node == u_input[i][j])[0][0]\n","                  v = np.where(node == u_input[i][k])[0][0]\n","                  u_A[u][v] +=2\n","            \n","            for i in range(len(u_input)):\n","              for kk in range(len(u_input[i])-1):\n","                jj=u_input[i][kk]\n","                ll=u_input[i][kk+1]\n","                if jj == 0 or ll == 0:\n","                  break\n","                u = np.where(node == jj)[0][0]\n","                v = np.where(node == ll)[0][0]\n","                u_A[u][v] +=1\n","              \n","            \n","        \n","            u_sum_in = np.sum(u_A, 0) # degree scaling\n","            u_sum_in[np.where(u_sum_in == 0)] = 1\n","            u_A_in = np.divide(u_A, u_sum_in)\n","            u_sum_out = np.sum(u_A, 1)\n","            u_sum_out[np.where(u_sum_out == 0)] = 1\n","            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n","            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n","            A.append(u_A)\n","            u_alias = []\n","            for aday in u_input:\n","              seq_aday = []\n","              for element in aday:\n","                seq_aday.append(np.where(node == element)[0][0])\n","              u_alias.append(seq_aday)\n","            alias_inputs.append(u_alias)\n","\n","        return alias_inputs, A, items, targets\n"],"metadata":{"id":"etHyRPaNqd6u","executionInfo":{"status":"ok","timestamp":1651760468859,"user_tz":-540,"elapsed":20,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":180,"outputs":[]},{"cell_type":"code","source":["train_data = tra\n","test_data = tes"],"metadata":{"id":"NICvaU8k6BiP","executionInfo":{"status":"ok","timestamp":1651760476907,"user_tz":-540,"elapsed":450,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":181,"outputs":[]},{"cell_type":"code","source":["train_data1 = Data(train_data,one_day_max=79,shuffle=False) # 같은 날짜 0 padding만 하는 것은 6분 정도.\n","test_data1 = Data(test_data,one_day_max=85,shuffle=False) # 같은 날짜 0 padding만 하는 것은 6분 정도."],"metadata":{"id":"NLF-J5Zc6EX3","executionInfo":{"status":"ok","timestamp":1651760484193,"user_tz":-540,"elapsed":1698,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":182,"outputs":[]},{"cell_type":"code","source":["class opt:\n","  def __init__(self,step,hidden_size,batch_size,nonhybrid=None):\n","    self.step=step\n","    self.hiddenSize = hidden_size\n","    #self.n_node = n_node\n","    self.batchSize = batch_size\n","    self.nonhybrid = nonhybrid\n","    self.lr = 0.001\n","    self.l2 = 1e-5\n","    self.lr_dc_step = 3\n","    self.lr_dc = 0.1\n","    self.epoch = 30\n","    self.patience = 10\n","oopt=opt(step = 1,hidden_size=100,batch_size=32)\n","n_node = 43849"],"metadata":{"id":"LvXL00eg74sL","executionInfo":{"status":"ok","timestamp":1651760490824,"user_tz":-540,"elapsed":789,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":183,"outputs":[]},{"cell_type":"code","source":["class GNN(tf.keras.layers.Layer):\n","    def __init__(self, hidden_size, step=1):\n","        super(GNN, self).__init__()\n","        self.step = step\n","        self.hidden_size = hidden_size\n","        self.input_size = hidden_size * 2\n","        self.gate_size = 3 * hidden_size\n","        # self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n","        self.stdv = 1/math.sqrt(self.hidden_size)\n","        self.w_ih = self.add_weight(shape=(self.input_size,self.gate_size),\n","                               initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),name='w_ih')\n","        '''\n","        AttributeError: module 'keras.api._v2.keras.initializers' has no attribute 'Uniform'\n","        add_weight doesn't have the attribute named 'kernel_initializer'\n","        '''\n","        # self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n","        self.w_hh = self.add_weight(shape=(self.hidden_size,self.gate_size),\n","                               initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),name = 'w_hh')\n","        # self.b_ih = Parameter(torch.Tensor(self.gate_size))\n","        self.b_ih = self.add_weight(shape=(self.gate_size,),\n","                               initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),name = 'b_ih')\n","        \n","        # self.b_hh = Parameter(torch.Tensor(self.gate_size))\n","        self.b_hh = self.add_weight(shape=(self.gate_size,),\n","                               initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),name = 'b_hh')\n","                \n","        # self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n","        self.b_iah = self.add_weight(shape=(self.hidden_size,),\n","                               initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),name = 'b_iah')\n","        # self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n","        self.b_oah = self.add_weight(shape=(self.hidden_size,),\n","                               initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),name = 'b_oah')\n","\n","        # self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_edge_in = tf.keras.layers.Dense(self.hidden_size, \n","                                                    use_bias=True,\n","                               kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                               bias_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv))\n","        '''\n","        TypeError: ('Unknown keyword argument:', 'bias') - > use_bias=True in Dense\n","        '''\n","        # self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_edge_out = tf.keras.layers.Dense(self.hidden_size,\n","                                    use_bias=True ,\n","                               kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                               bias_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv))\n","        # self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_edge_f = tf.keras.layers.Dense(self.hidden_size,\n","                                    use_bias=True ,\n","                               kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                               bias_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv))\n","\n","    def Cell(self, A, hidden):\n","        input_in = tf.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n","        input_out = tf.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]],self.linear_edge_out(hidden)) + self.b_oah\n","        inputs = tf.concat([input_in, input_out], 2) \n","        gi = tf.matmul(inputs, self.w_ih )+ self.b_ih # Dense layer와 같은 역할함.\n","        # gate_size가 앞서서 3* hidden_size로 정의되어 있었고 아래 chunk에서 split하는 것 같다.\n","        gh = tf.matmul(hidden, self.w_hh) + self.b_hh\n","        i_r, i_i, i_n = tf.split(gi,3,axis = 2)\n","        h_r, h_i, h_n = tf.split(gh,3,axis = 2)\n","        resetgate = tf.nn.sigmoid(i_r + h_r)\n","        inputgate = tf.nn.sigmoid(i_i + h_i)\n","        newgate = tf.nn.tanh(i_n + resetgate * h_n)\n","        hy = newgate + inputgate * (hidden - newgate)\n","        return hy\n","\n","    def call(self, A, hidden):\n","        for i in range(self.step):\n","            hidden = self.Cell(A, hidden)\n","        return hidden"],"metadata":{"id":"Akz9fULxCPab","executionInfo":{"status":"ok","timestamp":1651760493463,"user_tz":-540,"elapsed":3,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":184,"outputs":[]},{"cell_type":"code","source":["\n","class TAGNN(tf.keras.models.Model):\n","    def __init__(self, opt, n_node):\n","        super(TAGNN, self).__init__()\n","        self.hidden_size = opt.hiddenSize\n","        self.n_node = n_node\n","        self.batch_size = opt.batchSize\n","        self.nonhybrid = opt.nonhybrid\n","        # self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.embedding = tf.keras.layers.Embedding(input_dim = self.n_node,output_dim=self.hidden_size)\n","        self.stdv = 1/math.sqrt(self.hidden_size)\n","        \n","        self.gnn = GNN(self.hidden_size, step=opt.step)\n","\n","        # self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_one = tf.keras.layers.Dense(self.hidden_size, use_bias=True,\n","                                                kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                                                bias_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv))\n","        # self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_two = tf.keras.layers.Dense(self.hidden_size, use_bias=True,\n","                                                kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                                                bias_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv))\n","        # self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n","        self.linear_three = tf.keras.layers.Dense(1 , use_bias=False,\n","                                                  kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                                                  )\n","        # self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n","        \n","        self.linear_transform = tf.keras.layers.Dense(self.hidden_size, use_bias=True,\n","                                                      kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                                                      bias_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv))\n","\n","        # self.linear_t = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  #target attention\n","        self.linear_t = tf.keras.layers.Dense( self.hidden_size, use_bias=False,\n","                                              kernel_initializer=tf.keras.initializers.RandomUniform(-self.stdv,self.stdv),\n","                                              )\n","    def call(self, inputs, A,alias_inputs):\n","        if len(self.embedding.weights) != 0:\n","          norm = tf.norm(self.embedding.weights[0],axis=1) # 왜 여기도 하는지는 모르겠음.\n","          self.embedding.weights[0]=self.embedding.weights[0] / tf.reshape(norm,[-1,1])\n","        hidden = self.embedding(inputs) \n","        # inputs = items -> batch_size, len_seq_in_batch\n","        #print('NAN IN hidden')\n","        #print(tf.reduce_sum(hidden))\n","\n","\n","        hidden = self.gnn(A, hidden)\n","        #print('hidden after gnn')\n","        #print(hidden)\n","        \n","        def _get(j): # index로 slice해서 gather를 사용했음. \n","          return tf.gather(hidden[j],alias_inputs[j])\n","        \n","        get = lambda j : tf.gather(hidden[j],alias_inputs[j])\n","\n","        mask =tf.cast(tf.convert_to_tensor(alias_inputs) != 0,'float32')\n","        mask_new=mask[...,tf.newaxis]\n","\n","        mask=tf.cast(tf.reduce_sum(mask,axis =-1)!=0,'float32')\n","\n","\n","        \n","\n","\n","        #hidden= tf.reduce_sum(tf.stack([get(j) for j in range(len(alias_inputs))]) *mask_new,axis = 2)   # 이 줄에서 아마 retracing 일어나는듯 ? 이 부분에 대한 save에러도 있었음.\n","        hidden=tf.reduce_sum(tf.map_fn(_get,tf.range(0,alias_inputs.shape[0]), fn_output_signature = tf.float32)*mask_new,axis=2 ) # input과 dtype이 다르다면 반드시 fn_output_signature를 지정해주어야 한다.\n","\n","\n","        #hidden = tf.stack([get(j) for j in range(len(alias_inputs))])\n","        #hidden= tf.reduce_sum(tf.stack([get(j) for j in range(len(alias_inputs))]) *mask_new,axis = 2)   # 이 줄에서 아마 retracing 일어나는듯 ? 이 부분에 대한 save에러도 있었음.\n","        # https://stackoverflow.com/questions/62068323/iterating-over-tf-tensor-is-not-allowed-autograph-is-disabled-in-this-function\n","        # list comprehension을 제공하지 않는다.\n","\n","        #tf.reduce_sum(tf.stack([get(j) for j in range(len(alias_inputs))]) *mask_new,axis = 2)\n","\n","        # batch,max_len,one_day_max,hidden\n","\n","        #tf.zero(alias_inputs.shape[1],alias_inputs.shape[2],self.hidden_size)\n","\n","        #print(tf.map_fn(get_, hidden, dtype=tf.float32).shape)\n","\n","\n","        #print('tf.reduce_sum(tf.math.is_nan(hidden)) after tf.reduce......')\n","        #print(tf.reduce_sum(tf.cast(tf.math.is_nan(hidden),'float32')))\n","\n","        hidden = tf.math.divide_no_nan(hidden ,tf.reduce_sum(mask_new,axis =2))\n","        #print('tf.reduce_sum(tf.math.is_nan(hidden)) after divide...')\n","        #print(tf.reduce_sum(tf.cast(tf.math.is_nan(hidden),'float32')))\n","        seq_shape = hidden.shape.as_list() # 여기 해보아야 알듯 리스트로 차원반환하는 것 같은데\n","        \n","        # seq_hidden = seq_hidden.view(-1, model.hidden_size) # tf.reshape\n","\n","\n","        # 아마 이쯤부터 결측 생성되는 것 같음.-\n","        hidden = tf.reshape(hidden,[-1,self.hidden_size])\n","        # norms = torch.norm(seq_hidden, p=2, dim=1)  # tf.norm\n","        norms = tf.norm(hidden,axis =1)\n","        #print('tf.reduce_sum(tf.math.is_nan(norms))')\n","        #print(tf.reduce_sum(tf.cast(tf.math.is_nan(norms),'float32')))\n","        # seq_hidden = seq_hidden.div(norms.unsqueeze(-1).expand_as(seq_hidden)) # unsqueeze에 대해 알아보기\n","        \n","        #hidden = hidden / tf.reshape(norms,[-1,1])\n","        #print('tf.reduce_sum(tf.math.is_nan(hidden)) after hidden/...')\n","        #print(tf.reduce_sum(tf.cast(tf.math.is_nan(norms),'float32')))\n","        # expand_as는 broadcasting으로 해결될 수 있는지 확인해보기.\n","        # seq_hidden = seq_hidden.view(seq_shape) #  \n","        \n","        hidden = tf.reshape(hidden,[-1,seq_shape[1],seq_shape[2]])\n","\n","        #print(self.embedding.weights)\n","\n","\n","        ht = tf.gather_nd(hidden ,indices = tf.stack([tf.range(hidden.shape[0],dtype='int64') , tf.cast(tf.math.reduce_sum(mask,1),'int64') -1],axis=1))\n","        # tf.reshape을 이용해야함.\n","        #print('NAN IN hidden')\n","        #print(tf.reduce_sum(hidden))\n","        q1 = tf.reshape(self.linear_one(ht) ,[ht.shape[0], 1, ht.shape[1]])  # batch_size x 1 x latent_size\n","        #print('q1')\n","        #print(q1)\n","        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size # 여기가 결측의 시작인듯.\n","        #print('q2')\n","        #print(q2)\n","        alpha = self.linear_three(tf.nn.sigmoid(q1 + q2))  # (b,s,1)\n","        \n","        #print('alpha')\n","        #print(alpha)\n","        alpha = tf.nn.softmax(alpha, 1) # B,S,1\n","        #print('alpha')\n","        #print(alpha)\n","        #print(\"ht\") # 이거 문제 없음\n","        #print(ht)\n","        a = tf.math.reduce_sum(alpha * hidden * tf.cast(tf.reshape(mask, [mask.shape[0], -1, 1]),dtype = 'float32'), 1)  # (b,d)\n","        #print('a') # nan\n","        #print(a)\n","        #print('\\n')\n","        if not self.nonhybrid: # 여기 적용되는지 판단할 것\n","        \n","            a = self.linear_transform(tf.concat([a, ht], 1)) \n","        \n","        norm = tf.norm(self.embedding.weights[0],axis=1)\n","        self.embedding.weights[0]=self.embedding.weights[0] / tf.reshape(norm,[-1,1])\n","\n","        b = self.embedding.weights[0][1:] # 이 차원이 맞긴 함.\n","\n","        hidden = hidden * tf.cast(tf.reshape(mask,[mask.shape[0], -1, 1]),'float32')  # batch_size x seq_length x latent_size\n","        \n","        #print('hiddne')\n","\n","        qt = self.linear_t(hidden)  # batch_size x seq_length x latent_size\n","\n","        beta = tf.nn.softmax(b @ tf.transpose(qt,[0,2,1]), -1)  # batch_size x n_nodes x seq_length\n","        target = beta @ hidden  # batch_size x n_nodes x latent_size\n","        a = tf.reshape(a,[ht.shape[0], 1, ht.shape[1]])  # b,1,d\n","        a = a + target  # b,n,d # 이거 concat인가 ? \n","\n","        scores = tf.math.reduce_sum(a * b, -1)  # b,n\n","        scores=tf.nn.softmax(scores,-1)\n","\n","        return scores\n","    def get_config(self):\n","        return {\"hidden_units\": self.hidden_size}\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"],"metadata":{"id":"YSEw0uRgKK_3","executionInfo":{"status":"ok","timestamp":1651760496286,"user_tz":-540,"elapsed":4,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":185,"outputs":[]},{"cell_type":"markdown","source":["# NISER 논문에서 나온 것처럼 처음 모델이 gnn이전에 embedding layer의 weights을 normalize하는 것을 call 안에서 짜려고 했는데, 처음 embedding layer가 호출 되기 이전까지 weight이 존재하지 않아서 다른 방식으로 짜야할 것 같음.\n","\n","- build를 따로 둔다거나?\n","- Input을 사용해서 정의하는 것 ?\n","sequential이나\n","funtional api에서 하는 것처럼 일반적인 keras\n","built in을 사용하면 된다 ?\n","\n","\n","- 일단 미봉책으로 weight이 길이가 0인 경우 pass 한 다음 이후 embedding layer의 weights이 생겨난 후부터 normalize를 적용하도록 했음.\n","\n","\n","\n","```python\n","import numpy as np\n","\n","inputs = keras.Input(shape=(3,))\n","outputs = ActivityRegularizationLayer()(inputs)\n","model = keras.Model(inputs, outputs)\n","\n","# If there is a loss passed in `compile`, the regularization\n","# losses get added to it\n","model.compile(optimizer=\"adam\", loss=\"mse\")\n","model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n","\n","# It's also possible not to pass any loss in `compile`,\n","# since the model already has a loss to minimize, via the `add_loss`\n","# call during the forward pass!\n","model.compile(optimizer=\"adam\")\n","model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n","```"],"metadata":{"id":"UMyqcV3SxluL"}},{"cell_type":"code","source":["model=TAGNN(oopt,n_node)\n","\n","loss_ftn = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam(learning_rate = oopt.lr)\n","\n","\n","@tf.function\n","def train_ftn(item_input,A_input,alias_inputs_,targets_input):\n","    with tf.GradientTape() as tape :\n","      scores = model(item_input, A_input,alias_inputs_,training =True)\n","      #print(scores.numpy()) # 이거 nan 나옴.\n","      #print(targets_input)\n","      ll = loss_ftn(targets_input-1,scores)\n","\n","    \n","    \n","    gradients =tape.gradient(ll,model.trainable_weights)\n","\n","    optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n","    return ll"],"metadata":{"id":"GO2p3neZNt9y","executionInfo":{"status":"ok","timestamp":1651760502150,"user_tz":-540,"elapsed":475,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":186,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def test_ftn(item_input,A_input,alias_inputs_,targets_input):\n","    scores = model(item_input, A_input, alias_inputs_,training =False)\n","    scores = tf.math.top_k(scores,k=12).indices\n","    scores = tf.cast(scores,tf.int64)\n","    targets_input = tf.cast(targets_input,tf.int64)\n","\n","    #print(scores == targets_input[:,tf.newaxis]-1)\n","    result=tf.cast(scores == (targets_input[:,tf.newaxis]-1),tf.float32)\n","\n","    result_hit = tf.math.reduce_sum(result,axis =1)\n","    \n","    result_hit = tf.cast(result_hit != 0,tf.float32) # hit\n","\n","    index=tf.cast(tf.range(result.shape[-1])+1,tf.float32)\n","\n","\n","    result_mrr = tf.reduce_sum(result/tf.reshape(index,[-1,index.shape[0]]), axis =-1)\n","\n","\n","    return result_hit, result_mrr"],"metadata":{"id":"cMy8kbzSJ47O","executionInfo":{"status":"ok","timestamp":1651760502683,"user_tz":-540,"elapsed":2,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":187,"outputs":[]},{"cell_type":"code","source":["for epoch in range(oopt.epoch):\n","    print('-------------------------------------------------------')\n","    print('epoch: ', epoch)\n","\n","    slices = train_data1.generate_batch(model.batch_size)\n","    loss = 0\n","    for i, j in zip(slices, np.arange(len(slices))):\n","        alias_inputs, A, items, targets = train_data1.get_slice(i)\n","        \n","        alias_inputs = tf.convert_to_tensor(alias_inputs)\n","        items = tf.convert_to_tensor(items)\n","        \n","        A = tf.convert_to_tensor(A)\n","        \n","        targets = tf.convert_to_tensor(targets)\n","        \n","        l = train_ftn(items,A,alias_inputs,targets)\n","        \n","        loss += l \n","        \n","        if j % int(100) == 0:\n","            print('[%d/%d] Loss: %.4f' % (j, len(slices), l))\n","        \n","    \n","    \n","    print('\\tLoss:\\t%.3f' % loss)\n","    hit, mrr = tf.constant([],dtype= tf.float32), tf.constant([],dtype= tf.float32)\n","    hhit=[]\n","    mmrr=[]\n","    slices = test_data1.generate_batch(model.batch_size)\n","    for i in slices:\n","        alias_inputs, A, items, targets = test_data1.get_slice(i)\n","        \n","        alias_inputs = tf.convert_to_tensor(alias_inputs)\n","        items = tf.convert_to_tensor(items)\n","        \n","        A = tf.convert_to_tensor(A)\n","        \n","        targets = tf.convert_to_tensor(targets)\n","\n","        h,m=test_ftn(items,A,alias_inputs,targets)\n","\n","        hit = tf.concat([hit,h],0)\n","        mrr = tf.concat([mrr,m],0)\n","        '''\n","        scores = model(items, A, mask,alias_inputs,training =False)\n","        scores = tf.math.top_k(scores,k=20).indices\n","        scores = scores.numpy()\n","        \n","            # sub_scores를 tensor에서 numpy로 바꾸고 아래는 그대로 실행하면 됨.\n","        for score, target, mask in zip(scores, targets, test_data1.mask):\n","            hhit.append(np.isin(target - 1, score)) # 현재 target은 s_{n+1}인 item이므로, \n","                # score 즉, 위에서 추출한 index가 일치하면 hit이라는 list에 append 해준다.\n","\n","                # score vector에서 target-1과 일치하는 경우를 찾고, 없는 경우 0을 mrr이라는 list에 기록하고,\n","            if len(np.where(score == target - 1)[0]) == 0:\n","                mmrr.append(0)\n","            else:\n","                # 있는 경우에는 (1/그 위치의 index +1)을 기록한다.\n","                mmrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n","        '''\n","\n","    hit = np.mean(hit) * 100\n","    mrr = np.mean(mrr) * 100\n","\n","\n","    print(f\"Recall@12 : {hit} , MRR@12 {mrr}\")\n"],"metadata":{"id":"CHIeVds-wU0N","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"94109bc5-3bd9-41bf-8a32-070900b7a6ff","executionInfo":{"status":"error","timestamp":1651809344636,"user_tz":-540,"elapsed":5691,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":188,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["-------------------------------------------------------\n","epoch:  0\n","[0/198140] Loss: 10.6885\n","WARNING:tensorflow:5 out of the last 5 calls to <function train_ftn at 0x7efbedc90b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function train_ftn at 0x7efbedc90b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","[100/198140] Loss: 10.7026\n","[200/198140] Loss: 10.3085\n","[300/198140] Loss: 10.3244\n","[400/198140] Loss: 10.3200\n","[500/198140] Loss: 10.0283\n","[600/198140] Loss: 10.4715\n","[700/198140] Loss: 10.0880\n","[800/198140] Loss: 9.7910\n","[900/198140] Loss: 9.8641\n","[1000/198140] Loss: 10.3045\n","[1100/198140] Loss: 9.6231\n","[1200/198140] Loss: 10.0763\n","[1300/198140] Loss: 9.4903\n","[1400/198140] Loss: 11.0940\n","[1500/198140] Loss: 10.8475\n","[1600/198140] Loss: 9.9036\n","[1700/198140] Loss: 9.4160\n","[1800/198140] Loss: 10.2521\n","[1900/198140] Loss: 10.2454\n","[2000/198140] Loss: 10.1328\n","[2100/198140] Loss: 10.2841\n","[2200/198140] Loss: 10.0164\n","[2300/198140] Loss: 10.4383\n","[2400/198140] Loss: 10.3065\n","[2500/198140] Loss: 10.0739\n","[2600/198140] Loss: 10.1255\n","[2700/198140] Loss: 9.6939\n","[2800/198140] Loss: 10.3747\n","[2900/198140] Loss: 10.3827\n","[3000/198140] Loss: 9.6534\n","[3100/198140] Loss: 9.6303\n","[3200/198140] Loss: 9.7447\n","[3300/198140] Loss: 9.7642\n","[3400/198140] Loss: 9.6219\n","[3500/198140] Loss: 10.9123\n","[3600/198140] Loss: 10.3678\n","[3700/198140] Loss: 10.0344\n","[3800/198140] Loss: 10.4642\n","[3900/198140] Loss: 9.8158\n","[4000/198140] Loss: 9.2300\n","[4100/198140] Loss: 10.4900\n","[4200/198140] Loss: 10.1186\n","[4300/198140] Loss: 10.2290\n","[4400/198140] Loss: 10.6337\n","[4500/198140] Loss: 9.4184\n","[4600/198140] Loss: 10.6758\n","[4700/198140] Loss: 9.0927\n","[4800/198140] Loss: 9.9198\n","[4900/198140] Loss: 11.5299\n","[5000/198140] Loss: 10.2027\n","[5100/198140] Loss: 9.9403\n","[5200/198140] Loss: 10.5576\n","[5300/198140] Loss: 9.7943\n","[5400/198140] Loss: 9.3541\n","[5500/198140] Loss: 9.2751\n","[5600/198140] Loss: 10.4537\n","[5700/198140] Loss: 9.6677\n","[5800/198140] Loss: 10.3857\n","[5900/198140] Loss: 9.3185\n","[6000/198140] Loss: 9.8114\n","[6100/198140] Loss: 9.6949\n","[6200/198140] Loss: 9.8980\n","[6300/198140] Loss: 9.1151\n","[6400/198140] Loss: 9.5856\n","[6500/198140] Loss: 9.8245\n","[6600/198140] Loss: 9.6234\n","[6700/198140] Loss: 9.3092\n","[6800/198140] Loss: 9.7502\n","[6900/198140] Loss: 9.9350\n","[7000/198140] Loss: 9.2270\n","[7100/198140] Loss: 10.3094\n","[7200/198140] Loss: 9.6393\n","[7300/198140] Loss: 10.8005\n","[7400/198140] Loss: 10.1335\n","[7500/198140] Loss: 9.3199\n","[7600/198140] Loss: 9.8000\n","[7700/198140] Loss: 9.2681\n","[7800/198140] Loss: 9.7612\n","[7900/198140] Loss: 9.5443\n","[8000/198140] Loss: 9.0293\n","[8100/198140] Loss: 10.8457\n","[8200/198140] Loss: 9.8539\n","[8300/198140] Loss: 8.6963\n","[8400/198140] Loss: 10.2745\n","[8500/198140] Loss: 9.7678\n","[8600/198140] Loss: 9.1596\n","[8700/198140] Loss: 9.9165\n","[8800/198140] Loss: 10.0348\n","[8900/198140] Loss: 9.1320\n","[9000/198140] Loss: 8.8880\n","[9100/198140] Loss: 9.8171\n","[9200/198140] Loss: 10.5403\n","[9300/198140] Loss: 9.3750\n","[9400/198140] Loss: 9.7421\n","[9500/198140] Loss: 10.0130\n","[9600/198140] Loss: 9.8734\n","[9700/198140] Loss: 9.8792\n","[9800/198140] Loss: 10.0822\n","[9900/198140] Loss: 9.8971\n","[10000/198140] Loss: 9.2558\n","[10100/198140] Loss: 9.9000\n","[10200/198140] Loss: 9.4487\n","[10300/198140] Loss: 10.4107\n","[10400/198140] Loss: 9.9497\n","[10500/198140] Loss: 9.9064\n","[10600/198140] Loss: 9.9079\n","[10700/198140] Loss: 9.9266\n","[10800/198140] Loss: 8.9094\n","[10900/198140] Loss: 9.7859\n","[11000/198140] Loss: 9.9885\n","[11100/198140] Loss: 9.7647\n","[11200/198140] Loss: 9.9722\n","[11300/198140] Loss: 9.1459\n","[11400/198140] Loss: 9.5376\n","[11500/198140] Loss: 9.7558\n","[11600/198140] Loss: 9.8647\n","[11700/198140] Loss: 10.3817\n","[11800/198140] Loss: 10.2183\n","[11900/198140] Loss: 9.1408\n","[12000/198140] Loss: 8.6900\n","[12100/198140] Loss: 9.0569\n","[12200/198140] Loss: 9.4823\n","[12300/198140] Loss: 9.5079\n","[12400/198140] Loss: 9.2078\n","[12500/198140] Loss: 9.2694\n","[12600/198140] Loss: 10.2303\n","[12700/198140] Loss: 9.5666\n","[12800/198140] Loss: 9.3194\n","[12900/198140] Loss: 9.5352\n","[13000/198140] Loss: 9.9520\n","[13100/198140] Loss: 9.1805\n","[13200/198140] Loss: 10.0619\n","[13300/198140] Loss: 9.5375\n","[13400/198140] Loss: 9.2758\n","[13500/198140] Loss: 10.8695\n","[13600/198140] Loss: 9.6310\n","[13700/198140] Loss: 9.7310\n","[13800/198140] Loss: 9.2960\n","[13900/198140] Loss: 9.1860\n","[14000/198140] Loss: 8.7366\n","[14100/198140] Loss: 9.3043\n","[14200/198140] Loss: 8.9111\n","[14300/198140] Loss: 9.7905\n","[14400/198140] Loss: 8.4522\n","[14500/198140] Loss: 10.7409\n","[14600/198140] Loss: 8.5471\n","[14700/198140] Loss: 9.6478\n","[14800/198140] Loss: 9.9460\n","[14900/198140] Loss: 9.5906\n","[15000/198140] Loss: 9.6659\n","[15100/198140] Loss: 9.9348\n","[15200/198140] Loss: 9.8884\n","[15300/198140] Loss: 9.8842\n","[15400/198140] Loss: 9.6189\n","[15500/198140] Loss: 10.0154\n","[15600/198140] Loss: 9.3926\n","[15700/198140] Loss: 10.1027\n","[15800/198140] Loss: 9.7756\n","[15900/198140] Loss: 10.0557\n","[16000/198140] Loss: 10.1934\n","[16100/198140] Loss: 9.4498\n","[16200/198140] Loss: 10.3856\n","[16300/198140] Loss: 9.8551\n","[16400/198140] Loss: 9.6111\n","[16500/198140] Loss: 9.6381\n","[16600/198140] Loss: 10.0052\n","[16700/198140] Loss: 9.7364\n","[16800/198140] Loss: 9.6405\n","[16900/198140] Loss: 9.3517\n","[17000/198140] Loss: 9.8417\n","[17100/198140] Loss: 9.5575\n","[17200/198140] Loss: 10.4627\n","[17300/198140] Loss: 9.0175\n","[17400/198140] Loss: 9.5129\n","[17500/198140] Loss: 9.8776\n","[17600/198140] Loss: 10.0459\n","[17700/198140] Loss: 9.6134\n","[17800/198140] Loss: 9.6475\n","[17900/198140] Loss: 9.1941\n","[18000/198140] Loss: 9.1703\n","[18100/198140] Loss: 9.6436\n","[18200/198140] Loss: 10.0192\n","[18300/198140] Loss: 10.0477\n","[18400/198140] Loss: 9.3045\n","[18500/198140] Loss: 9.1604\n","[18600/198140] Loss: 9.4248\n","[18700/198140] Loss: 10.2355\n","[18800/198140] Loss: 9.8771\n","[18900/198140] Loss: 10.7725\n","[19000/198140] Loss: 9.8341\n","[19100/198140] Loss: 9.7525\n","[19200/198140] Loss: 9.6860\n","[19300/198140] Loss: 10.3334\n","[19400/198140] Loss: 9.3714\n","[19500/198140] Loss: 9.1856\n","[19600/198140] Loss: 9.1100\n","[19700/198140] Loss: 10.1356\n","[19800/198140] Loss: 9.7427\n","[19900/198140] Loss: 9.1729\n","[20000/198140] Loss: 10.0428\n","[20100/198140] Loss: 9.7283\n","[20200/198140] Loss: 9.6936\n","[20300/198140] Loss: 10.2411\n","[20400/198140] Loss: 10.2579\n","[20500/198140] Loss: 9.1643\n","[20600/198140] Loss: 9.9393\n","[20700/198140] Loss: 10.3049\n","[20800/198140] Loss: 9.3410\n","[20900/198140] Loss: 9.1077\n","[21000/198140] Loss: 10.1801\n","[21100/198140] Loss: 9.2044\n","[21200/198140] Loss: 9.5159\n","[21300/198140] Loss: 10.4618\n","[21400/198140] Loss: 9.1990\n","[21500/198140] Loss: 9.5484\n","[21600/198140] Loss: 9.2376\n","[21700/198140] Loss: 9.6039\n","[21800/198140] Loss: 9.2124\n","[21900/198140] Loss: 9.6334\n","[22000/198140] Loss: 9.7650\n","[22100/198140] Loss: 10.3320\n","[22200/198140] Loss: 8.8416\n","[22300/198140] Loss: 9.4391\n","[22400/198140] Loss: 11.2066\n","[22500/198140] Loss: 10.0998\n","[22600/198140] Loss: 10.0244\n","[22700/198140] Loss: 9.1850\n","[22800/198140] Loss: 9.9059\n","[22900/198140] Loss: 9.8104\n","[23000/198140] Loss: 10.2423\n","[23100/198140] Loss: 10.6183\n","[23200/198140] Loss: 10.0256\n","[23300/198140] Loss: 9.0294\n","[23400/198140] Loss: 10.2485\n","[23500/198140] Loss: 9.4957\n","[23600/198140] Loss: 9.6642\n","[23700/198140] Loss: 9.3231\n","[23800/198140] Loss: 9.6841\n","[23900/198140] Loss: 9.7914\n","[24000/198140] Loss: 9.0826\n","[24100/198140] Loss: 8.9557\n","[24200/198140] Loss: 9.1362\n","[24300/198140] Loss: 9.3819\n","[24400/198140] Loss: 9.5297\n","[24500/198140] Loss: 9.1074\n","[24600/198140] Loss: 8.6529\n","[24700/198140] Loss: 9.2823\n","[24800/198140] Loss: 11.9349\n","[24900/198140] Loss: 10.0562\n","[25000/198140] Loss: 9.6149\n","[25100/198140] Loss: 9.3798\n","[25200/198140] Loss: 8.9419\n","[25300/198140] Loss: 9.3353\n","[25400/198140] Loss: 9.5010\n","[25500/198140] Loss: 9.1877\n","[25600/198140] Loss: 9.1032\n","[25700/198140] Loss: 9.0251\n","[25800/198140] Loss: 10.1125\n","[25900/198140] Loss: 9.4062\n","[26000/198140] Loss: 9.9323\n","[26100/198140] Loss: 9.9865\n","[26200/198140] Loss: 10.3221\n","[26300/198140] Loss: 9.5537\n","[26400/198140] Loss: 9.0352\n","[26500/198140] Loss: 10.1538\n","[26600/198140] Loss: 9.3646\n","[26700/198140] Loss: 9.2064\n","[26800/198140] Loss: 8.9743\n","[26900/198140] Loss: 9.7951\n","[27000/198140] Loss: 9.9273\n","[27100/198140] Loss: 9.7386\n","[27200/198140] Loss: 9.1047\n","[27300/198140] Loss: 10.0599\n","[27400/198140] Loss: 10.3759\n","[27500/198140] Loss: 9.9793\n","[27600/198140] Loss: 9.7106\n","[27700/198140] Loss: 8.8733\n","[27800/198140] Loss: 10.5987\n","[27900/198140] Loss: 9.2572\n","[28000/198140] Loss: 9.0967\n","[28100/198140] Loss: 10.2864\n","[28200/198140] Loss: 9.4412\n","[28300/198140] Loss: 8.4071\n","[28400/198140] Loss: 8.9792\n","[28500/198140] Loss: 9.4520\n","[28600/198140] Loss: 9.4615\n","[28700/198140] Loss: 8.3875\n","[28800/198140] Loss: 9.2564\n","[28900/198140] Loss: 10.0544\n","[29000/198140] Loss: 9.0775\n","[29100/198140] Loss: 9.1039\n","[29200/198140] Loss: 9.7066\n","[29300/198140] Loss: 9.2180\n","[29400/198140] Loss: 9.7315\n","[29500/198140] Loss: 9.6607\n","[29600/198140] Loss: 9.8848\n","[29700/198140] Loss: 10.0292\n","[29800/198140] Loss: 9.5027\n","[29900/198140] Loss: 9.2876\n","[30000/198140] Loss: 9.6379\n","[30100/198140] Loss: 9.5206\n","[30200/198140] Loss: 9.9018\n","[30300/198140] Loss: 9.6717\n","[30400/198140] Loss: 9.0446\n","[30500/198140] Loss: 8.8823\n","[30600/198140] Loss: 9.6809\n","[30700/198140] Loss: 9.9671\n","[30800/198140] Loss: 9.5659\n","[30900/198140] Loss: 8.9792\n","[31000/198140] Loss: 9.4372\n","[31100/198140] Loss: 9.6516\n","[31200/198140] Loss: 9.9902\n","[31300/198140] Loss: 8.8494\n","[31400/198140] Loss: 9.8615\n","[31500/198140] Loss: 11.0048\n","[31600/198140] Loss: 9.6258\n","[31700/198140] Loss: 9.5249\n","[31800/198140] Loss: 10.3242\n","[31900/198140] Loss: 9.5492\n","[32000/198140] Loss: 8.9587\n","[32100/198140] Loss: 9.4133\n","[32200/198140] Loss: 9.7592\n","[32300/198140] Loss: 9.5263\n","[32400/198140] Loss: 9.3245\n","[32500/198140] Loss: 9.1450\n","[32600/198140] Loss: 9.6998\n","[32700/198140] Loss: 8.8399\n","[32800/198140] Loss: 9.2759\n","[32900/198140] Loss: 9.6576\n","[33000/198140] Loss: 9.5693\n","[33100/198140] Loss: 9.1753\n","[33200/198140] Loss: 9.3362\n","[33300/198140] Loss: 9.4577\n","[33400/198140] Loss: 8.9370\n","[33500/198140] Loss: 9.0290\n","[33600/198140] Loss: 9.2820\n","[33700/198140] Loss: 10.5887\n","[33800/198140] Loss: 9.3372\n","[33900/198140] Loss: 9.8386\n","[34000/198140] Loss: 9.3575\n","[34100/198140] Loss: 9.2958\n","[34200/198140] Loss: 9.6958\n","[34300/198140] Loss: 10.0171\n","[34400/198140] Loss: 9.2330\n","[34500/198140] Loss: 7.7729\n","[34600/198140] Loss: 9.2905\n","[34700/198140] Loss: 9.2777\n","[34800/198140] Loss: 10.1919\n","[34900/198140] Loss: 9.8391\n","[35000/198140] Loss: 8.7294\n","[35100/198140] Loss: 9.7855\n","[35200/198140] Loss: 9.1189\n","[35300/198140] Loss: 9.9254\n","[35400/198140] Loss: 9.6890\n","[35500/198140] Loss: 9.7162\n","[35600/198140] Loss: 8.5527\n","[35700/198140] Loss: 10.9548\n","[35800/198140] Loss: 9.6362\n","[35900/198140] Loss: 9.2417\n","[36000/198140] Loss: 9.2929\n","[36100/198140] Loss: 9.9374\n","[36200/198140] Loss: 8.9196\n","[36300/198140] Loss: 9.4202\n","[36400/198140] Loss: 9.1480\n","[36500/198140] Loss: 9.1335\n","[36600/198140] Loss: 10.0384\n","[36700/198140] Loss: 9.0605\n","[36800/198140] Loss: 9.4538\n","[36900/198140] Loss: 9.1379\n","[37000/198140] Loss: 9.5445\n","[37100/198140] Loss: 9.1234\n","[37200/198140] Loss: 9.8993\n","[37300/198140] Loss: 10.8655\n","[37400/198140] Loss: 8.8476\n","[37500/198140] Loss: 9.1311\n","[37600/198140] Loss: 9.4174\n","[37700/198140] Loss: 10.2950\n","[37800/198140] Loss: 8.7260\n","[37900/198140] Loss: 9.5603\n","[38000/198140] Loss: 9.4417\n","[38100/198140] Loss: 10.3992\n","[38200/198140] Loss: 9.2385\n","[38300/198140] Loss: 8.9085\n","[38400/198140] Loss: 8.8135\n","[38500/198140] Loss: 8.7547\n","[38600/198140] Loss: 9.5105\n","[38700/198140] Loss: 9.8284\n","[38800/198140] Loss: 9.3806\n","[38900/198140] Loss: 9.2221\n","[39000/198140] Loss: 9.0393\n","[39100/198140] Loss: 9.4638\n","[39200/198140] Loss: 9.7574\n","[39300/198140] Loss: 9.4756\n","[39400/198140] Loss: 9.4677\n","[39500/198140] Loss: 9.1050\n","[39600/198140] Loss: 9.2988\n","[39700/198140] Loss: 9.9933\n","[39800/198140] Loss: 9.5492\n","[39900/198140] Loss: 9.4358\n","[40000/198140] Loss: 8.9940\n","[40100/198140] Loss: 10.7775\n","[40200/198140] Loss: 9.6542\n","[40300/198140] Loss: 9.3447\n","[40400/198140] Loss: 9.5449\n","[40500/198140] Loss: 8.9916\n","[40600/198140] Loss: 8.9763\n","[40700/198140] Loss: 9.7097\n","[40800/198140] Loss: 9.0196\n","[40900/198140] Loss: 9.4668\n","[41000/198140] Loss: 8.8077\n","[41100/198140] Loss: 9.6191\n","[41200/198140] Loss: 9.6733\n","[41300/198140] Loss: 9.3086\n","[41400/198140] Loss: 9.1391\n","[41500/198140] Loss: 9.5820\n","[41600/198140] Loss: 10.0526\n","[41700/198140] Loss: 9.5097\n","[41800/198140] Loss: 9.0416\n","[41900/198140] Loss: 10.2007\n","[42000/198140] Loss: 9.0604\n","[42100/198140] Loss: 9.1542\n","[42200/198140] Loss: 9.5449\n","[42300/198140] Loss: 8.8357\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-188-eec7b589946a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0malias_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0malias_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-180-61645950edb8>\u001b[0m in \u001b[0;36mget_slice\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     93\u001b[0m               \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0mu_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m  \u001b[0mu_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model.save_weights('./model/0505_niser_200901_256')"],"metadata":{"id":"V1d2JEQMJPa_","executionInfo":{"status":"ok","timestamp":1651809344644,"user_tz":-540,"elapsed":11,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":189,"outputs":[]},{"cell_type":"code","source":["#model.save('./model/0502_niser_200901_256')"],"metadata":{"id":"7Fsp2GWgSOMJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```\n","-------------------------------------------------------\n","epoch:  0\n","[0/4402] Loss: 9.1327\n","WARNING:tensorflow:5 out of the last 6 calls to <function train_ftn at 0x7f80038c4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 7 calls to <function train_ftn at 0x7f80038c4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","[100/4402] Loss: 8.9770\n","[200/4402] Loss: 8.9180\n","[300/4402] Loss: 8.4044\n","[400/4402] Loss: 8.6265\n","[500/4402] Loss: 8.7719\n","[600/4402] Loss: 8.4934\n","[700/4402] Loss: 9.0608\n","[800/4402] Loss: 8.4043\n","[900/4402] Loss: 8.3754\n","[1000/4402] Loss: 9.1329\n","[1100/4402] Loss: 8.7070\n","[1200/4402] Loss: 8.5734\n","[1300/4402] Loss: 8.1753\n","[1400/4402] Loss: 8.4076\n","[1500/4402] Loss: 8.9500\n","[1600/4402] Loss: 8.3972\n","[1700/4402] Loss: 8.4945\n","[1800/4402] Loss: 8.4742\n","[1900/4402] Loss: 9.1580\n","[2000/4402] Loss: 8.6381\n","[2100/4402] Loss: 8.2778\n","[2200/4402] Loss: 8.4068\n","[2300/4402] Loss: 8.7677\n","[2400/4402] Loss: 8.1248\n","[2500/4402] Loss: 8.5282\n","[2600/4402] Loss: 8.0610\n","[2700/4402] Loss: 8.3762\n","[2800/4402] Loss: 8.6859\n","[2900/4402] Loss: 8.6202\n","[3000/4402] Loss: 8.6322\n","[3100/4402] Loss: 8.5665\n","[3200/4402] Loss: 8.2761\n","[3300/4402] Loss: 8.9086\n","[3400/4402] Loss: 8.2455\n","[3500/4402] Loss: 8.8695\n","[3600/4402] Loss: 8.3441\n","[3700/4402] Loss: 8.5327\n","[3800/4402] Loss: 8.3098\n","[3900/4402] Loss: 7.8990\n","[4000/4402] Loss: 8.0932\n","[4100/4402] Loss: 8.0460\n","[4200/4402] Loss: 8.4132\n","[4300/4402] Loss: 8.2627\n","[4400/4402] Loss: 8.4437\n","\tLoss:\t37235.625\n","Recall@20 : 4.538934305310249 , MRR@20 1.084378082305193\n","-------------------------------------------------------\n","epoch:  1\n","[0/4402] Loss: 8.4622\n","[100/4402] Loss: 8.6117\n","[200/4402] Loss: 7.8981\n","[300/4402] Loss: 7.8315\n","[400/4402] Loss: 8.1448\n","[500/4402] Loss: 8.5316\n","[600/4402] Loss: 7.9694\n","[700/4402] Loss: 8.6572\n","[800/4402] Loss: 8.2223\n","[900/4402] Loss: 8.0679\n","[1000/4402] Loss: 8.7123\n","[1100/4402] Loss: 8.0266\n","[1200/4402] Loss: 8.2721\n","[1300/4402] Loss: 7.8984\n","[1400/4402] Loss: 8.2625\n","[1500/4402] Loss: 8.5136\n","[1600/4402] Loss: 7.8585\n","[1700/4402] Loss: 8.0175\n","[1800/4402] Loss: 8.0727\n","[1900/4402] Loss: 8.6581\n","[2000/4402] Loss: 8.3162\n","[2100/4402] Loss: 8.0461\n","[2200/4402] Loss: 8.0951\n","[2300/4402] Loss: 8.3961\n","[2400/4402] Loss: 7.7614\n","[2500/4402] Loss: 8.1474\n","[2600/4402] Loss: 7.4720\n","[2700/4402] Loss: 7.8181\n","[2800/4402] Loss: 8.3181\n","[2900/4402] Loss: 8.2480\n","[3000/4402] Loss: 8.0860\n","[3100/4402] Loss: 8.2640\n","[3200/4402] Loss: 7.6628\n","[3300/4402] Loss: 8.1966\n","[3400/4402] Loss: 7.6164\n","[3500/4402] Loss: 8.6833\n","[3600/4402] Loss: 7.8270\n","[3700/4402] Loss: 8.4333\n","[3800/4402] Loss: 8.0896\n","[3900/4402] Loss: 7.2534\n","[4000/4402] Loss: 7.7906\n","[4100/4402] Loss: 7.5586\n","[4200/4402] Loss: 8.1443\n","[4300/4402] Loss: 8.0703\n","[4400/4402] Loss: 7.9771\n","\tLoss:\t35420.930\n","Recall@20 : 6.506147235631943 , MRR@20 2.2530045360326767\n","-------------------------------------------------------\n","epoch:  2\n","[0/4402] Loss: 8.4803\n","[100/4402] Loss: 8.3959\n","[200/4402] Loss: 7.4060\n","[300/4402] Loss: 7.3159\n","[400/4402] Loss: 7.9019\n","[500/4402] Loss: 8.1200\n","[600/4402] Loss: 7.6013\n","[700/4402] Loss: 8.6809\n","[800/4402] Loss: 7.8981\n","[900/4402] Loss: 7.1553\n","[1000/4402] Loss: 8.0226\n","[1100/4402] Loss: 6.9836\n","[1200/4402] Loss: 8.0866\n","[1300/4402] Loss: 7.7664\n","[1400/4402] Loss: 8.0761\n","[1500/4402] Loss: 8.0080\n","[1600/4402] Loss: 7.3935\n","[1700/4402] Loss: 7.7209\n","[1800/4402] Loss: 7.5963\n","[1900/4402] Loss: 8.2674\n","[2000/4402] Loss: 7.9365\n","[2100/4402] Loss: 7.5936\n","[2200/4402] Loss: 7.7149\n","[2300/4402] Loss: 8.1713\n","[2400/4402] Loss: 7.4022\n","[2500/4402] Loss: 7.5430\n","[2600/4402] Loss: 6.9785\n","[2700/4402] Loss: 7.4260\n","[2800/4402] Loss: 7.6647\n","[2900/4402] Loss: 7.8535\n","[3000/4402] Loss: 7.3364\n","[3100/4402] Loss: 7.8327\n","[3200/4402] Loss: 7.2999\n","[3300/4402] Loss: 7.5044\n","[3400/4402] Loss: 7.0310\n","[3500/4402] Loss: 8.3287\n","[3600/4402] Loss: 7.2841\n","[3700/4402] Loss: 8.2101\n","[3800/4402] Loss: 7.8499\n","[3900/4402] Loss: 6.6918\n","[4000/4402] Loss: 7.5366\n","[4100/4402] Loss: 6.9652\n","[4200/4402] Loss: 7.8374\n","[4300/4402] Loss: 7.8099\n","[4400/4402] Loss: 7.4924\n","\tLoss:\t33645.664\n","Recall@20 : 7.079917937517166 , MRR@20 2.9857616871595383\n","-------------------------------------------------------\n","epoch:  3\n","[0/4402] Loss: 8.5211\n","[100/4402] Loss: 8.3228\n","[200/4402] Loss: 7.1613\n","[300/4402] Loss: 6.9613\n","[400/4402] Loss: 7.6526\n","[500/4402] Loss: 7.7041\n","[600/4402] Loss: 7.2721\n","[700/4402] Loss: 7.9722\n","[800/4402] Loss: 7.4510\n","[900/4402] Loss: 6.4149\n","[1000/4402] Loss: 7.6176\n","[1100/4402] Loss: 6.1612\n","[1200/4402] Loss: 7.9348\n","[1300/4402] Loss: 7.5432\n","[1400/4402] Loss: 7.8400\n","[1500/4402] Loss: 7.5729\n","[1600/4402] Loss: 6.8761\n","[1700/4402] Loss: 7.6372\n","[1800/4402] Loss: 7.4500\n","[1900/4402] Loss: 7.8619\n","[2000/4402] Loss: 7.6442\n","[2100/4402] Loss: 6.9684\n","[2200/4402] Loss: 7.3588\n","[2300/4402] Loss: 7.9117\n","[2400/4402] Loss: 7.0074\n","[2500/4402] Loss: 7.1272\n","[2600/4402] Loss: 6.6058\n","[2700/4402] Loss: 7.0542\n","[2800/4402] Loss: 7.2066\n","[2900/4402] Loss: 7.5580\n","[3000/4402] Loss: 6.6977\n","[3100/4402] Loss: 7.4124\n","[3200/4402] Loss: 6.9918\n","[3300/4402] Loss: 6.9365\n","[3400/4402] Loss: 6.4698\n","[3500/4402] Loss: 8.0780\n","[3600/4402] Loss: 6.8603\n","[3700/4402] Loss: 7.9404\n","[3800/4402] Loss: 7.6608\n","[3900/4402] Loss: 6.3413\n","[4000/4402] Loss: 7.2477\n","[4100/4402] Loss: 6.4833\n","[4200/4402] Loss: 7.5917\n","[4300/4402] Loss: 7.3635\n","[4400/4402] Loss: 6.9842\n","\tLoss:\t32015.068\n","Recall@20 : 6.8954914808273315 , MRR@20 3.0188381671905518\n","```"],"metadata":{"id":"TQmH0ngOJQON"}},{"cell_type":"markdown","source":["```\n","epoch:  0\n","[0/551] Loss: 9.1313\n","WARNING:tensorflow:5 out of the last 6 calls to <function train_ftn at 0x7fde94691320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 7 calls to <function train_ftn at 0x7fde94691320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","[100/551] Loss: 8.5320\n","[200/551] Loss: 8.4573\n","[300/551] Loss: 8.3493\n","[400/551] Loss: 8.4255\n","[500/551] Loss: 8.4161\n","\tLoss:\t4682.043\n","WARNING:tensorflow:5 out of the last 5 calls to <function test_ftn at 0x7fde9a7d6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function test_ftn at 0x7fde9a7d6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","Recall@20 : 3.678278625011444 , MRR@20 0.8045623078942299\n","-------------------------------------------------------\n","epoch:  1\n","[0/551] Loss: 8.4360\n","[100/551] Loss: 8.3471\n","[200/551] Loss: 8.3401\n","[300/551] Loss: 8.1966\n","[400/551] Loss: 8.2209\n","[500/551] Loss: 8.2677\n","\tLoss:\t4583.788\n","Recall@20 : 3.9036884903907776 , MRR@20 0.9260150603950024\n","-------------------------------------------------------\n","epoch:  2\n","[0/551] Loss: 8.4063\n","[100/551] Loss: 8.2909\n","[200/551] Loss: 8.2130\n","[300/551] Loss: 8.0507\n","[400/551] Loss: 8.0739\n","[500/551] Loss: 8.0964\n","\tLoss:\t4504.420\n","Recall@20 : 3.6885246634483337 , MRR@20 0.7741636596620083\n","-------------------------------------------------------\n","epoch:  3\n","[0/551] Loss: 8.3050\n","[100/551] Loss: 8.2216\n","[200/551] Loss: 8.0289\n","[300/551] Loss: 7.9143\n","[400/551] Loss: 7.9192\n","[500/551] Loss: 7.9483\n","\tLoss:\t4433.559\n","Recall@20 : 3.6270491778850555 , MRR@20 0.8505591191351414\n","-------------------------------------------------------\n","epoch:  4\n","[0/551] Loss: 8.1300\n","[100/551] Loss: 8.1082\n","[200/551] Loss: 7.8838\n","[300/551] Loss: 7.7846\n","[400/551] Loss: 7.8147\n","[500/551] Loss: 7.7884\n","\tLoss:\t4363.283\n","Recall@20 : 4.231557250022888 , MRR@20 1.0650495998561382\n","-------------------------------------------------------\n","epoch:  5\n","[0/551] Loss: 8.0003\n","[100/551] Loss: 7.9730\n","[200/551] Loss: 7.7583\n","[300/551] Loss: 7.6875\n","[400/551] Loss: 7.6993\n","[500/551] Loss: 7.6322\n","\tLoss:\t4289.802\n","Recall@20 : 4.651639237999916 , MRR@20 1.255041267722845\n","-------------------------------------------------------\n","epoch:  6\n","[0/551] Loss: 7.8406\n","[100/551] Loss: 7.8541\n","[200/551] Loss: 7.6583\n","[300/551] Loss: 7.5677\n","[400/551] Loss: 7.5432\n","[500/551] Loss: 7.4900\n","\tLoss:\t4214.937\n","Recall@20 : 5.010245740413666 , MRR@20 1.4170462265610695\n","-------------------------------------------------------\n","epoch:  7\n","[0/551] Loss: 7.7513\n","[100/551] Loss: 7.6584\n","[200/551] Loss: 7.5355\n","[300/551] Loss: 7.3945\n","[400/551] Loss: 7.4355\n","[500/551] Loss: 7.3660\n","\tLoss:\t4139.040\n","Recall@20 : 5.922131240367889 , MRR@20 1.7022568732500076\n","-------------------------------------------------------\n","epoch:  8\n","[0/551] Loss: 7.6751\n","[100/551] Loss: 7.5035\n","[200/551] Loss: 7.3307\n","[300/551] Loss: 7.2344\n","[400/551] Loss: 7.2386\n","[500/551] Loss: 7.2018\n","\tLoss:\t4056.599\n","Recall@20 : 6.014344096183777 , MRR@20 1.9783595576882362\n","-------------------------------------------------------\n","epoch:  9\n","[0/551] Loss: 7.5049\n","[100/551] Loss: 7.3761\n","[200/551] Loss: 7.1566\n","[300/551] Loss: 7.1323\n","[400/551] Loss: 7.0531\n","[500/551] Loss: 7.0586\n","\tLoss:\t3971.185\n","Recall@20 : 6.106557324528694 , MRR@20 2.1189233288168907\n","-------------------------------------------------------\n","epoch:  10\n","[0/551] Loss: 7.3151\n","[100/551] Loss: 7.1911\n","[200/551] Loss: 6.9490\n","[300/551] Loss: 6.9698\n","[400/551] Loss: 6.8792\n","[500/551] Loss: 6.8710\n","\tLoss:\t3884.150\n","Recall@20 : 6.055327877402306 , MRR@20 2.176481857895851\n","-------------------------------------------------------\n","epoch:  11\n","[0/551] Loss: 7.1611\n","[100/551] Loss: 7.0065\n","[200/551] Loss: 6.7958\n","[300/551] Loss: 6.8369\n","[400/551] Loss: 6.6549\n","[500/551] Loss: 6.7200\n","\tLoss:\t3800.317\n","Recall@20 : 6.086065620183945 , MRR@20 2.1360553801059723\n","-------------------------------------------------------\n","epoch:  12\n","[0/551] Loss: 7.0522\n","[100/551] Loss: 6.8564\n","[200/551] Loss: 6.6179\n","[300/551] Loss: 6.6923\n","[400/551] Loss: 6.5266\n","[500/551] Loss: 6.6744\n","\tLoss:\t3720.019\n","Recall@20 : 6.045081838965416 , MRR@20 2.235601842403412\n","-------------------------------------------------------\n","epoch:  13\n","[0/551] Loss: 6.9531\n","[100/551] Loss: 6.7291\n","[200/551] Loss: 6.4314\n","[300/551] Loss: 6.5536\n","[400/551] Loss: 6.3977\n","[500/551] Loss: 6.5429\n","\tLoss:\t3639.909\n","Recall@20 : 6.116803362965584 , MRR@20 2.2891057655215263\n","-------------------------------------------------------\n","epoch:  14\n","[0/551] Loss: 6.8361\n","[100/551] Loss: 6.5514\n","[200/551] Loss: 6.2841\n","\n","```"],"metadata":{"id":"8KwhJi_EyexS"}},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"O5QauQ-pwWy7","executionInfo":{"status":"ok","timestamp":1651809363378,"user_tz":-540,"elapsed":991,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":190,"outputs":[]},{"cell_type":"code","source":["open('HandM/0501for_submission_tes.txt', 'rb')"],"metadata":{"id":"4HuwPwXEnfYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub2=pickle.load(open('./HandM/0505for_submission_tes.txt', 'rb'))"],"metadata":{"id":"fQT-bjoemaNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub1=pickle.load(open('HandM/0506for_submission_tra.txt', 'rb'))\n","item_dict=pickle.load(open('HandM/0506item_dict.txt', 'rb'))"],"metadata":{"id":"yLz8x6hAm-ME","executionInfo":{"status":"ok","timestamp":1651809404514,"user_tz":-540,"elapsed":10653,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":193,"outputs":[]},{"cell_type":"code","source":["len(sub1[0])"],"metadata":{"id":"Gtz0dNNW-ZES","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651809404514,"user_tz":-540,"elapsed":7,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}},"outputId":"dd76ae63-e505-4ec0-d7d5-25b1e2f3d7ee"},"execution_count":194,"outputs":[{"output_type":"execute_result","data":{"text/plain":["514472"]},"metadata":{},"execution_count":194}]},{"cell_type":"code","source":["train_sub=sub_Data(sub1,one_day_max=79)\n","slices = train_sub.generate_batch(model.batch_size)"],"metadata":{"id":"8UKieXO2yGUJ","executionInfo":{"status":"ok","timestamp":1651809435598,"user_tz":-540,"elapsed":732,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":197,"outputs":[]},{"cell_type":"code","source":["out = []\n","for i, j in zip(slices, np.arange(len(slices))):\n","    alias_inputs, A, items = train_sub.preprocess(i)\n","    \n","    alias_inputs = tf.convert_to_tensor(alias_inputs)\n","    items = tf.convert_to_tensor(items)\n","    A = tf.convert_to_tensor(A)\n","    targets = tf.convert_to_tensor(targets)\n","    scores = model(items, A, alias_inputs,training =False)\n","    scores = tf.math.top_k(scores,k=12).indices\n","    scores = tf.cast(scores,tf.int64) # # 메모리 터짐\n","    out.append(scores)\n","    if j % 100 ==0 : print((j / len(slices))*100)"],"metadata":{"id":"BMmyU1QwvVBK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c428cbcd-c0da-46ab-f096-ab58564c30df"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.0\n","0.621967906456027\n","1.243935812912054\n","1.8659037193680807\n","2.487871625824108\n","3.109839532280134\n","3.7318074387361615\n","4.353775345192188\n","4.975743251648216\n","5.597711158104242\n","6.219679064560268\n","6.841646971016295\n","7.463614877472323\n","8.08558278392835\n","8.707550690384377\n","9.329518596840403\n","9.951486503296431\n","10.573454409752458\n","11.195422316208484\n","11.81739022266451\n","12.439358129120537\n","13.061326035576565\n"]}]},{"cell_type":"code","source":["out=tf.concat(out,axis=0)\n","out=out+1\n","\n","out_dict={j:i for i,j in item_dict.items()}\n","\n","out_mapped=np.vectorize(out_dict.get)(out)\n","\n","out1=pd.DataFrame(out_mapped)\n","out1=\"0\"+out1\n","\n","out2=pd.DataFrame(out1.to_string(header=False,\n","                  index=False,\n","                  index_names=False).split('\\n'))"],"metadata":{"id":"d3cy5suVw5Y3","executionInfo":{"status":"ok","timestamp":1651747257707,"user_tz":-540,"elapsed":468,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["out2['customer_id'] = sub1[0]\n","out2.columns = [\"out\",'customer_id']"],"metadata":{"id":"F1UMCLnZ-KUU","executionInfo":{"status":"ok","timestamp":1651747352107,"user_tz":-540,"elapsed":1017,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":135,"outputs":[]},{"cell_type":"code","source":["submission=pd.read_csv('./HandM/submission.csv')"],"metadata":{"id":"FWI61MEr_3rT","executionInfo":{"status":"ok","timestamp":1651747375511,"user_tz":-540,"elapsed":4203,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":138,"outputs":[]},{"cell_type":"code","source":["submission.loc[submission.customer_id.isin(out2.customer_id) ,'prediction'] = out2['out'].values"],"metadata":{"id":"Ty5RjVGoBsFG","executionInfo":{"status":"ok","timestamp":1651747390716,"user_tz":-540,"elapsed":454,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":139,"outputs":[]},{"cell_type":"code","source":["submission.to_csv('HandM/0505_niser_submission.csv',index=False)"],"metadata":{"id":"wFwaGYEkCJSc","executionInfo":{"status":"ok","timestamp":1651747428350,"user_tz":-540,"elapsed":8821,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":141,"outputs":[]},{"cell_type":"code","source":["!pip install kaggle"],"metadata":{"id":"y4PO8fkFDZVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f ./HandM/0505_niser_submission.csv -m \"Message\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCD7fstpDStL","executionInfo":{"status":"ok","timestamp":1651747453798,"user_tz":-540,"elapsed":23507,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}},"outputId":"af6d72c0-ea0b-4f9c-f818-bbee80073a29"},"execution_count":142,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 258M/258M [00:21<00:00, 12.6MB/s]\n","Successfully submitted to H&M Personalized Fashion Recommendations"]}]},{"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))"],"metadata":{"id":"x34FsmdCDTY8","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":94},"executionInfo":{"status":"ok","timestamp":1651743285897,"user_tz":-540,"elapsed":12974,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}},"outputId":"b4901625-f16b-4977-dc5f-cee7e700b447"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-5e2c1cfc-1a5c-45dd-8c56-c87fd9b0d62e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5e2c1cfc-1a5c-45dd-8c56-c87fd9b0d62e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","User uploaded file \"kaggle.json\" with length 67 bytes\n"]}]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"GDrec4KmEmYc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class sub_Data():\n","    def __init__(self, data, one_day_max=100,shuffle=False, graph=None):\n","        inputs = data[1]\n","        len_max = max_len_check(inputs)\n","        self.inputs = inputs\n","        \n","        self.len_max = len_max\n","        self.length = len(inputs)\n","        self.shuffle = shuffle\n","        self.graph = graph\n","        self.one_day_max = one_day_max\n","\n","    def generate_batch(self, batch_size):\n","        if self.shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.inputs = self.inputs[shuffled_arg]\n","            #self.mask = self.mask[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n","        return slices\n","        \n","    def preprocess(self,i):\n","        #inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n","        inputs = []\n","        for jk in i :\n","          inputs.append(self.inputs[jk])\n","\n","        us_lens = [len(upois) for upois in inputs] # 이게 오래 걸리는듯.\n","        \n","        \n","        inputs = [[j + [0] * (self.one_day_max - len(j)) for j in one_seq] for one_seq in inputs]\n","        \n","        \n","        for idx,le in enumerate(us_lens):\n","          for jj in range(self.len_max - le):\n","            inputs[idx].append(self.one_day_max * [0])\n","        \n","\n","        \n","        \n","        items, n_node, A, alias_inputs = [], [], [], []\n","        for u_input in inputs:\n","            n_node.append(len(np.unique(u_input))) \n","        max_n_node = np.max(n_node) # 해당 slices 중 한 sequence 안에서 가질 수 있는 아이템의 종류의 최댓값\n","        # 0 패딩 후에 개수를 따지는 것이기 때문에 id 개수 +1 임.\n","        \n","        \n","        \n","        for u_input in inputs:\n","            node = np.unique(u_input)\n","            \n","           \n","            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n","            \n","            u_A = np.zeros((max_n_node, max_n_node)) # 균일한 크기로 생성.\n","            \n","            \n","        \n","\n","            for i in range(len(u_input)):\n","              # same time point loop\n","              length=len(u_input[i])\n","              for j in range(length):\n","                for k in range(length):\n","                  if u_input[i][k] == 0 or  u_input[i][j] == 0:\n","                    break\n","                  u = np.where(node == u_input[i][j])[0][0]\n","                  v = np.where(node == u_input[i][k])[0][0]\n","                  u_A[u][v] +=2\n","            \n","            for i in range(len(u_input)):\n","              for kk in range(len(u_input[i])-1):\n","                jj=u_input[i][kk]\n","                ll=u_input[i][kk+1]\n","                if jj == 0 or ll == 0:\n","                  break\n","                u = np.where(node == jj)[0][0]\n","                v = np.where(node == ll)[0][0]\n","                u_A[u][v] +=1\n","              \n","            \n","        \n","            u_sum_in = np.sum(u_A, 0) # degree scaling\n","            u_sum_in[np.where(u_sum_in == 0)] = 1\n","            u_A_in = np.divide(u_A, u_sum_in)\n","            u_sum_out = np.sum(u_A, 1)\n","            u_sum_out[np.where(u_sum_out == 0)] = 1\n","            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n","            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n","            A.append(u_A)\n","            u_alias = []\n","            for aday in u_input:\n","              seq_aday = []\n","              for element in aday:\n","                seq_aday.append(np.where(node == element)[0][0])\n","              u_alias.append(seq_aday)\n","            alias_inputs.append(u_alias)\n","\n","        return alias_inputs, A, items"],"metadata":{"id":"jPqT9NS2xngC","executionInfo":{"status":"ok","timestamp":1651809408359,"user_tz":-540,"elapsed":592,"user":{"displayName":"SangHyuk Lee","userId":"04957292248187568065"}}},"execution_count":195,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"maj845MdxucR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# data set을 늘리는 방안도 고려해야함."],"metadata":{"id":"tBaAbdkJi3a3"}},{"cell_type":"markdown","source":["### padding part를 get_slices 내부에 넣었음. -> 매 epoch 마다 했던 padding 연산을 다시함. -> 메모리 문제는 더 이상 없어졌으나 굉장히 소모적으로 변함.\n","\n","### 이에 disk 용량을 사용해서 즉, 미리 padding된 객체를 생성해서 disk에 넣고 꺼내 쓰는 방식을 사용하고자 함."],"metadata":{"id":"PahemhWePw6Q"}},{"cell_type":"markdown","source":["# 0501 \n","\n","- 데이터 획득시점 : 2020-09-01\n","- test set 설정 방법 : 한 sequence를 기준으로 첫 시점이 09-15인 경우를 test set으로 구분.\n","- duplicates -> drop 했었음."],"metadata":{"id":"UQeMljG2mmST"}},{"cell_type":"markdown","source":["```\n","\n","Recall@20 : 0.17144919838756323 , MRR@20 0.02397021889919415\n","-------------------------------------------------------\n","epoch:  1\n","[0/1396] Loss: 10.7619\n","[100/1396] Loss: 8.1350\n","[200/1396] Loss: 6.0566\n","[300/1396] Loss: 7.1981\n","[400/1396] Loss: 7.2146\n","[500/1396] Loss: 5.8306\n","[600/1396] Loss: 5.2453\n","[700/1396] Loss: 3.4625\n","[800/1396] Loss: 2.1029\n","[900/1396] Loss: 1.6762\n","[1000/1396] Loss: 1.8171\n","[1100/1396] Loss: 0.0002\n","[1200/1396] Loss: 0.0019\n","[1300/1396] Loss: 0.0001\n","\tLoss:\t5437.556\n","Recall@20 : 0.41508753784000874 , MRR@20 0.07311432273127139\n","-------------------------------------------------------\n","epoch:  2\n","[0/1396] Loss: 9.3718\n","[100/1396] Loss: 7.0771\n","[200/1396] Loss: 6.2432\n","[300/1396] Loss: 5.6623\n","[400/1396] Loss: 5.6069\n","[500/1396] Loss: 4.0980\n","[600/1396] Loss: 3.0256\n","[700/1396] Loss: 0.9985\n","[800/1396] Loss: 1.1898\n","[900/1396] Loss: 1.1581\n","[1000/1396] Loss: 0.1014\n","[1100/1396] Loss: 0.0008\n","[1200/1396] Loss: 0.0034\n","[1300/1396] Loss: 0.0003\n","\tLoss:\t3991.758\n","Recall@20 : 0.7309149950742722 , MRR@20 0.1114008016884327\n","-------------------------------------------------------\n","epoch:  3\n","[0/1396] Loss: 8.3455\n","[100/1396] Loss: 5.6436\n","[200/1396] Loss: 4.2199\n","[300/1396] Loss: 4.1691\n","[400/1396] Loss: 2.5864\n","[500/1396] Loss: 1.6439\n","[600/1396] Loss: 0.3421\n","[700/1396] Loss: 0.3205\n","[800/1396] Loss: 0.1477\n","[900/1396] Loss: 0.1285\n","[1000/1396] Loss: 0.0224\n","[1100/1396] Loss: 0.0011\n","[1200/1396] Loss: 0.0010\n","[1300/1396] Loss: 0.0001\n","\tLoss:\t2508.844\n","Recall@20 : 2.0573902875185013 , MRR@20 0.5278557073324919\n","-------------------------------------------------------\n","epoch:  4\n","[0/1396] Loss: 8.9669\n","[100/1396] Loss: 4.1573\n","[200/1396] Loss: 2.3965\n","[300/1396] Loss: 1.6168\n","[400/1396] Loss: 0.8023\n","[500/1396] Loss: 0.1551\n","[600/1396] Loss: 0.0367\n","[700/1396] Loss: 0.0039\n","[800/1396] Loss: 0.1776\n","[900/1396] Loss: 0.0233\n","[1000/1396] Loss: 0.0059\n","[1100/1396] Loss: 0.0002\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t1298.375\n","Recall@20 : 3.73578779399395 , MRR@20 1.3103222474455833\n","-------------------------------------------------------\n","epoch:  5\n","[0/1396] Loss: 7.4700\n","[100/1396] Loss: 1.1910\n","[200/1396] Loss: 0.6365\n","[300/1396] Loss: 0.6646\n","[400/1396] Loss: 0.3910\n","[500/1396] Loss: 1.1450\n","[600/1396] Loss: 0.0403\n","[700/1396] Loss: 0.0214\n","[800/1396] Loss: 0.1528\n","[900/1396] Loss: 0.0063\n","[1000/1396] Loss: 0.0035\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t731.314\n","Recall@20 : 4.9720268696546555 , MRR@20 2.0377062261104584\n","-------------------------------------------------------\n","epoch:  6\n","[0/1396] Loss: 5.9955\n","[100/1396] Loss: 1.5992\n","[200/1396] Loss: 0.4148\n","[300/1396] Loss: 0.7427\n","[400/1396] Loss: 0.5668\n","[500/1396] Loss: 0.3221\n","[600/1396] Loss: 0.0057\n","[700/1396] Loss: 0.0009\n","[800/1396] Loss: 0.1897\n","[900/1396] Loss: 0.0274\n","[1000/1396] Loss: 0.0020\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0007\n","[1300/1396] Loss: 0.0001\n","\tLoss:\t649.514\n","Recall@20 : 5.161523073911667 , MRR@20 2.151380106806755\n","-------------------------------------------------------\n","epoch:  7\n","[0/1396] Loss: 4.4476\n","[100/1396] Loss: 1.6693\n","[200/1396] Loss: 1.6100\n","[300/1396] Loss: 0.9982\n","[400/1396] Loss: 0.9884\n","[500/1396] Loss: 0.2785\n","[600/1396] Loss: 0.2758\n","[700/1396] Loss: 0.0462\n","[800/1396] Loss: 0.2909\n","[900/1396] Loss: 0.0534\n","[1000/1396] Loss: 0.0020\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0008\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t705.316\n","Recall@20 : 6.64140060544014 , MRR@20 3.0510060489177704\n","-------------------------------------------------------\n","epoch:  8\n","[0/1396] Loss: 5.4679\n","[100/1396] Loss: 1.2573\n","[200/1396] Loss: 0.7012\n","[300/1396] Loss: 2.2229\n","[400/1396] Loss: 1.1801\n","[500/1396] Loss: 0.1892\n","[600/1396] Loss: 0.0034\n","[700/1396] Loss: 0.0001\n","[800/1396] Loss: 0.1617\n","[900/1396] Loss: 0.0008\n","[1000/1396] Loss: 0.0018\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t612.357\n","Recall@20 : 6.876014918088913 , MRR@20 3.1907230615615845\n","-------------------------------------------------------\n","epoch:  9\n","[0/1396] Loss: 3.2161\n","[100/1396] Loss: 0.9919\n","[200/1396] Loss: 0.4371\n","[300/1396] Loss: 1.0255\n","[400/1396] Loss: 0.5027\n","[500/1396] Loss: 0.8706\n","[600/1396] Loss: 0.0030\n","[700/1396] Loss: 0.0007\n","[800/1396] Loss: 0.1721\n","[900/1396] Loss: 0.3536\n","[1000/1396] Loss: 0.0005\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t583.171\n","Recall@20 : 7.444504648447037 , MRR@20 3.347522020339966\n","-------------------------------------------------------\n","epoch:  10\n","[0/1396] Loss: 3.3454\n","[100/1396] Loss: 1.1065\n","[200/1396] Loss: 0.4173\n","[300/1396] Loss: 0.9339\n","[400/1396] Loss: 0.2872\n","[500/1396] Loss: 0.1755\n","[600/1396] Loss: 0.0104\n","[700/1396] Loss: 0.0109\n","[800/1396] Loss: 0.2029\n","[900/1396] Loss: 0.0005\n","[1000/1396] Loss: 0.0008\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0002\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t554.232\n","Recall@20 : 7.579859346151352 , MRR@20 3.700995445251465\n","-------------------------------------------------------\n","epoch:  11\n","[0/1396] Loss: 3.1046\n","[100/1396] Loss: 1.4957\n","[200/1396] Loss: 0.3188\n","[300/1396] Loss: 0.6736\n","[400/1396] Loss: 0.2668\n","[500/1396] Loss: 0.1906\n","[600/1396] Loss: 0.0987\n","[700/1396] Loss: 0.0018\n","[800/1396] Loss: 0.3331\n","[900/1396] Loss: 0.0033\n","[1000/1396] Loss: 0.0206\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t539.397\n","Recall@20 : 7.11965337395668 , MRR@20 3.4687120467424393\n","-------------------------------------------------------\n","epoch:  12\n","[0/1396] Loss: 2.8918\n","[100/1396] Loss: 1.6235\n","[200/1396] Loss: 0.3028\n","[300/1396] Loss: 1.7735\n","[400/1396] Loss: 0.5591\n","[500/1396] Loss: 0.2507\n","[600/1396] Loss: 0.0001\n","[700/1396] Loss: 0.0052\n","[800/1396] Loss: 0.2073\n","[900/1396] Loss: 0.0053\n","[1000/1396] Loss: 0.0026\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t506.588\n","Recall@20 : 7.236960530281067 , MRR@20 3.569217026233673\n","-------------------------------------------------------\n","epoch:  13\n","[0/1396] Loss: 2.9235\n","[100/1396] Loss: 0.6941\n","[200/1396] Loss: 0.5572\n","[300/1396] Loss: 1.0340\n","[400/1396] Loss: 0.4004\n","[500/1396] Loss: 0.4084\n","[600/1396] Loss: 0.0006\n","[700/1396] Loss: 0.0079\n","[800/1396] Loss: 0.1796\n","[900/1396] Loss: 0.1135\n","[1000/1396] Loss: 0.0009\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t493.460\n","Recall@20 : 7.209889590740204 , MRR@20 3.4221820533275604\n","-------------------------------------------------------\n","epoch:  14\n","[0/1396] Loss: 3.4950\n","[100/1396] Loss: 0.9645\n","[200/1396] Loss: 0.3847\n","[300/1396] Loss: 0.7333\n","[400/1396] Loss: 0.4559\n","[500/1396] Loss: 0.8811\n","[600/1396] Loss: 0.0075\n","[700/1396] Loss: 0.0000\n","[800/1396] Loss: 0.1666\n","[900/1396] Loss: 0.0002\n","[1000/1396] Loss: 0.0003\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t482.189\n","Recall@20 : 7.2550080716609955 , MRR@20 3.655760735273361\n","-------------------------------------------------------\n","epoch:  15\n","[0/1396] Loss: 2.9218\n","[100/1396] Loss: 0.7126\n","[200/1396] Loss: 0.3196\n","[300/1396] Loss: 0.5116\n","[400/1396] Loss: 0.5754\n","[500/1396] Loss: 0.1065\n","[600/1396] Loss: 0.0001\n","[700/1396] Loss: 0.0033\n","[800/1396] Loss: 0.1723\n","[900/1396] Loss: 0.0192\n","[1000/1396] Loss: 0.0007\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t475.000\n","Recall@20 : 7.579859346151352 , MRR@20 3.734191507101059\n","-------------------------------------------------------\n","epoch:  16\n","[0/1396] Loss: 3.0302\n","[100/1396] Loss: 0.6599\n","[200/1396] Loss: 0.7916\n","[300/1396] Loss: 0.8719\n","[400/1396] Loss: 0.1834\n","[500/1396] Loss: 0.1710\n","[600/1396] Loss: 0.0151\n","[700/1396] Loss: 0.0103\n","[800/1396] Loss: 0.2309\n","[900/1396] Loss: 0.0020\n","[1000/1396] Loss: 0.0009\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t455.962\n","Recall@20 : 8.15737247467041 , MRR@20 3.932087868452072\n","-------------------------------------------------------\n","epoch:  17\n","[0/1396] Loss: 3.2356\n","[100/1396] Loss: 0.7728\n","[200/1396] Loss: 0.4321\n","[300/1396] Loss: 0.9849\n","[400/1396] Loss: 0.2544\n","[500/1396] Loss: 0.2003\n","[600/1396] Loss: 0.0005\n","[700/1396] Loss: 0.0006\n","[800/1396] Loss: 0.4181\n","[900/1396] Loss: 0.1147\n","[1000/1396] Loss: 0.0005\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t441.683\n","Recall@20 : 7.715214043855667 , MRR@20 3.807774931192398\n","-------------------------------------------------------\n","epoch:  18\n","[0/1396] Loss: 3.3083\n","[100/1396] Loss: 0.7006\n","[200/1396] Loss: 0.4273\n","[300/1396] Loss: 1.0539\n","[400/1396] Loss: 0.2600\n","[500/1396] Loss: 0.4774\n","[600/1396] Loss: 0.0251\n","[700/1396] Loss: 0.0011\n","[800/1396] Loss: 0.3265\n","[900/1396] Loss: 0.0000\n","[1000/1396] Loss: 0.0006\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t426.165\n","Recall@20 : 8.15737247467041 , MRR@20 3.850197419524193\n","-------------------------------------------------------\n","epoch:  19\n","[0/1396] Loss: 2.8038\n","[100/1396] Loss: 0.6553\n","[200/1396] Loss: 0.3520\n","[300/1396] Loss: 0.5705\n","[400/1396] Loss: 0.4822\n","[500/1396] Loss: 0.6115\n","[600/1396] Loss: 0.0010\n","[700/1396] Loss: 0.0377\n","[800/1396] Loss: 0.2922\n","[900/1396] Loss: 0.0085\n","[1000/1396] Loss: 0.0004\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t420.555\n","Recall@20 : 7.462552189826965 , MRR@20 3.706730529665947\n","-------------------------------------------------------\n","epoch:  20\n","[0/1396] Loss: 2.9215\n","[100/1396] Loss: 1.0358\n","[200/1396] Loss: 0.3942\n","[300/1396] Loss: 0.6650\n","[400/1396] Loss: 0.3501\n","[500/1396] Loss: 0.1506\n","[600/1396] Loss: 0.2228\n","[700/1396] Loss: 0.0003\n","[800/1396] Loss: 0.1668\n","[900/1396] Loss: 0.0068\n","[1000/1396] Loss: 0.0009\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t417.759\n","Recall@20 : 7.5979068875312805 , MRR@20 3.7883199751377106\n","-------------------------------------------------------\n","epoch:  21\n","[0/1396] Loss: 2.9510\n","[100/1396] Loss: 0.6082\n","[200/1396] Loss: 0.2534\n","[300/1396] Loss: 0.9363\n","[400/1396] Loss: 0.3094\n","[500/1396] Loss: 0.5843\n","[600/1396] Loss: 0.0002\n","[700/1396] Loss: 0.0001\n","[800/1396] Loss: 0.1751\n","[900/1396] Loss: 0.1381\n","[1000/1396] Loss: 0.0007\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t407.660\n","Recall@20 : 7.128676772117615 , MRR@20 3.7243787199258804\n","-------------------------------------------------------\n","epoch:  22\n","[0/1396] Loss: 2.9615\n","[100/1396] Loss: 1.0033\n","[200/1396] Loss: 0.5204\n","[300/1396] Loss: 0.5787\n","[400/1396] Loss: 0.4209\n","[500/1396] Loss: 0.2499\n","[600/1396] Loss: 0.0001\n","[700/1396] Loss: 0.0024\n","[800/1396] Loss: 0.2402\n","[900/1396] Loss: 0.0000\n","[1000/1396] Loss: 0.0006\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t413.572\n","Recall@20 : 7.426457107067108 , MRR@20 3.6325834691524506\n","-------------------------------------------------------\n","epoch:  23\n","[0/1396] Loss: 2.7388\n","[100/1396] Loss: 0.6911\n","[200/1396] Loss: 0.4048\n","[300/1396] Loss: 0.5865\n","[400/1396] Loss: 0.3651\n","[500/1396] Loss: 0.2849\n","[600/1396] Loss: 0.0006\n","[700/1396] Loss: 0.0082\n","[800/1396] Loss: 0.1567\n","[900/1396] Loss: 0.0012\n","[1000/1396] Loss: 0.0000\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t388.499\n","Recall@20 : 7.688143104314804 , MRR@20 3.889022395014763\n","-------------------------------------------------------\n","epoch:  24\n","[0/1396] Loss: 2.5770\n","[100/1396] Loss: 0.6097\n","[200/1396] Loss: 0.2550\n","[300/1396] Loss: 1.3593\n","[400/1396] Loss: 0.4429\n","[500/1396] Loss: 0.1640\n","[600/1396] Loss: 0.0028\n","[700/1396] Loss: 0.0016\n","[800/1396] Loss: 0.1537\n","[900/1396] Loss: 0.1958\n","[1000/1396] Loss: 0.0014\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0003\n","[1300/1396] Loss: 0.0001\n","\tLoss:\t392.554\n","Recall@20 : 7.372315227985382 , MRR@20 3.489958494901657\n","-------------------------------------------------------\n","epoch:  25\n","[0/1396] Loss: 2.9666\n","[100/1396] Loss: 0.6565\n","[200/1396] Loss: 1.0836\n","[300/1396] Loss: 0.7324\n","[400/1396] Loss: 0.4313\n","[500/1396] Loss: 0.2031\n","[600/1396] Loss: 0.0044\n","[700/1396] Loss: 0.0097\n","[800/1396] Loss: 0.1731\n","[900/1396] Loss: 0.0004\n","[1000/1396] Loss: 0.0010\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t388.989\n","Recall@20 : 7.128676772117615 , MRR@20 3.722606599330902\n","-------------------------------------------------------\n","epoch:  26\n","[0/1396] Loss: 2.7439\n","[100/1396] Loss: 1.1669\n","[200/1396] Loss: 0.4727\n","[300/1396] Loss: 0.5670\n","[400/1396] Loss: 0.2277\n","[500/1396] Loss: 0.2410\n","[600/1396] Loss: 0.0001\n","[700/1396] Loss: 0.0001\n","[800/1396] Loss: 0.2020\n","[900/1396] Loss: 0.0005\n","[1000/1396] Loss: 0.0004\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t389.218\n","Recall@20 : 7.4715755879879 , MRR@20 3.7830282002687454\n","-------------------------------------------------------\n","epoch:  27\n","[0/1396] Loss: 2.5396\n","[100/1396] Loss: 0.6617\n","[200/1396] Loss: 0.3004\n","[300/1396] Loss: 0.6330\n","[400/1396] Loss: 0.1818\n","[500/1396] Loss: 0.0997\n","[600/1396] Loss: 0.0096\n","[700/1396] Loss: 0.0049\n","[800/1396] Loss: 0.1628\n","[900/1396] Loss: 0.0228\n","[1000/1396] Loss: 0.0002\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t369.752\n","Recall@20 : 7.137700915336609 , MRR@20 3.5162944346666336\n","-------------------------------------------------------\n","epoch:  28\n","[0/1396] Loss: 2.2986\n","[100/1396] Loss: 0.7124\n","[200/1396] Loss: 0.3090\n","[300/1396] Loss: 0.6825\n","[400/1396] Loss: 0.2792\n","[500/1396] Loss: 0.4574\n","[600/1396] Loss: 0.0020\n","[700/1396] Loss: 0.0010\n","[800/1396] Loss: 0.1708\n","[900/1396] Loss: 0.0009\n","[1000/1396] Loss: 0.0001\n","[1100/1396] Loss: 0.0001\n","[1200/1396] Loss: 0.0000\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t365.582\n","Recall@20 : 7.291102409362793 , MRR@20 3.6785967648029327\n","-------------------------------------------------------\n","epoch:  29\n","[0/1396] Loss: 2.6138\n","[100/1396] Loss: 0.9707\n","[200/1396] Loss: 0.2130\n","[300/1396] Loss: 0.7378\n","[400/1396] Loss: 0.6036\n","[500/1396] Loss: 0.1433\n","[600/1396] Loss: 0.0004\n","[700/1396] Loss: 0.0102\n","[800/1396] Loss: 0.2366\n","[900/1396] Loss: 0.2245\n","[1000/1396] Loss: 0.0001\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0001\n","[1300/1396] Loss: 0.0000\n","\tLoss:\t364.168\n","Recall@20 : 7.426457107067108 , MRR@20 3.912309929728508\n","```"],"metadata":{"id":"rT_nz5-3kALM"}},{"cell_type":"markdown","source":["```\n","[0/1396] Loss: 9.3718\n","[100/1396] Loss: 7.0771\n","[200/1396] Loss: 6.2432\n","[300/1396] Loss: 5.6623\n","[400/1396] Loss: 5.6069\n","[500/1396] Loss: 4.0980\n","[600/1396] Loss: 3.0256\n","[700/1396] Loss: 0.9985\n","[800/1396] Loss: 1.1898\n","[900/1396] Loss: 1.1581\n","[1000/1396] Loss: 0.1014\n","[1100/1396] Loss: 0.0008\n","[1200/1396] Loss: 0.0034\n","[1300/1396] Loss: 0.0003\n","\tLoss:\t3991.758\n","Recall@20 : 0.7309149950742722 , MRR@20 0.1114008016884327\n","```"],"metadata":{"id":"TdKySraGqnZx"}},{"cell_type":"markdown","source":["- 두번째 epoch.\n","```\n","[0/1396] Loss: 10.7619\n","[100/1396] Loss: 8.1350\n","[200/1396] Loss: 6.0566\n","[300/1396] Loss: 7.1981\n","[400/1396] Loss: 7.2146\n","[500/1396] Loss: 5.8306\n","[600/1396] Loss: 5.2453\n","[700/1396] Loss: 3.4625\n","[800/1396] Loss: 2.1029\n","[900/1396] Loss: 1.6762\n","[1000/1396] Loss: 1.8171\n","[1100/1396] Loss: 0.0002\n","[1200/1396] Loss: 0.0019\n","[1300/1396] Loss: 0.0001\n","\tLoss:\t5437.55\n","Recall@20 : 0.41508753784000874 , MRR@20 0.07311432273127139\n","```"],"metadata":{"id":"8tDDvMrzko7V"}},{"cell_type":"markdown","source":["```\n","0501 첫번째 epoch\n","\n","[200/1396] Loss: 8.8053\n","[300/1396] Loss: 8.8630\n","[400/1396] Loss: 8.9722\n","[500/1396] Loss: 8.8889\n","[600/1396] Loss: 8.8052\n","[700/1396] Loss: 8.5113\n","[800/1396] Loss: 6.5697\n","[900/1396] Loss: 7.6722\n","[1000/1396] Loss: 6.6189\n","[1100/1396] Loss: 0.0000\n","[1200/1396] Loss: 0.0019\n","[1300/1396] Loss: 0.0000\n","\n","Recall@20 : 0.17144919838756323 , MRR@20 0.02397021889919415\n","```\n"],"metadata":{"id":"okhD0YR8hYcD"}},{"cell_type":"code","source":["for i, j in zip(slices, np.arange(len(slices))):\n","    alias_inputs, A, items, targets = train_data1.get_slice(i)"],"metadata":{"id":"KhG0JFdzA0ma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alias_inputs, A, items, targets = train_data1.get_slice(i)"],"metadata":{"id":"9l4E6Q1FO-oP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targets -1"],"metadata":{"id":"Umhyx2wlSnXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, j in zip(slices, np.arange(len(slices))):\n","  pass"],"metadata":{"id":"sKQ7r2i0SSrQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i"],"metadata":{"id":"RiYC35akSkzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Node: 'tagnn_42/embedding_42/embedding_lookup'\n","indices[79,1] = 9608 is not in [0, 9608)\n","\t [[{{node tagnn_42/embedding_42/embedding_lookup}}]] [Op:__inference_train_ftn_1000891]\n"],"metadata":{"id":"hXH_NlKMEGjM"}},{"cell_type":"code","source":["mrr"],"metadata":{"id":"tebOybnam7-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.mean(hhit)"],"metadata":{"id":"0st_EHPKm5g9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mmrr"],"metadata":{"id":"AwC9RomemtEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    scores = tf.math.top_k(scores,k=20).indices\n","    scores = tf.cast(scores,tf.int64)\n","    scores = tf.cast(targets,tf.int64)\n","    \n","    result=tf.cast(scores == targets[:,tf.newaxis]-1,tf.float32)"],"metadata":{"id":"xVLfgmNwkBey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"id":"jvuOPMkhkJ5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hit=[]\n","mrr=[]"],"metadata":{"id":"sPW4R_BOkSEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#        scores = model(items, A, mask,alias_inputs,training =False)\n","scores = tf.math.top_k(scores,k=20).indices\n","scores = scores.numpy()\n","for score, target, mask in zip(scores, targets, test_data1.mask):\n","    hit.append(np.isin(target - 1, score)) # 현재 target은 s_{n+1}인 item이므로, \n","        # score 즉, 위에서 추출한 index가 일치하면 hit이라는 list에 append 해준다.\n","\n","        # score vector에서 target-1과 일치하는 경우를 찾고, 없는 경우 0을 mrr이라는 list에 기록하고,\n","    if len(np.where(score == target - 1)[0]) == 0:\n","        mrr.append(0)\n","    else:\n","        # 있는 경우에는 (1/그 위치의 index +1)을 기록한다.\n","        mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))"],"metadata":{"id":"QbcYLikTYUj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hit"],"metadata":{"id":"I1-luX-ZZRLr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1회 epoch\n","```\n"," Recall@20 : 45.50432957593784 , MRR@20 15.308210672538506\n","```\n","# 2회 epoch\n","\n","```\n","Recall@20 : 51.77839242775932 , MRR@20 17.690032606197565\n","```\n","\n","# 3회 epoch\n","```\n","Recall@20 : 52.64569752734653 , MRR@20 18.16359554471299\n","```\n","\n","Recall@20 : 51.98409940650757 , MRR@20 18.082734233101373"],"metadata":{"id":"Kz7mEdNVIOZ-"}}],"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.9"},"orig_nbformat":4,"colab":{"name":"NISER_v1_4_handm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}